{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read Input Data\n",
    "\n",
    "# # Patient 1\n",
    "# cgm_df = pd.read_csv(\"data/CGMData.csv\", parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "# insulin_df = pd.read_csv(\"data/InsulinData.csv\", parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "\n",
    "\n",
    "# Patient 2\n",
    "# cgm_df = pd.read_excel(\"data/CGMData670GPatient3.xlsx\", parse_dates=[['Date', 'Time']])\n",
    "# insulin_df = pd.read_excel(\"data/InsulinAndMealIntake670GPatient3.xlsx\", parse_dates=[['Date', 'Time']])\n",
    "\n",
    "\n",
    "# cgm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter only the needed fields\n",
    "\n",
    "def extract_meal_and_no_meal_instances(cgm_data_file, insulin_data_file):\n",
    "    if \".xls\" in cgm_data_file:\n",
    "        cgm_df = pd.read_excel(cgm_data_file, parse_dates=[['Date', 'Time']])\n",
    "    elif \".csv\" in cgm_data_file:\n",
    "        cgm_df = pd.read_csv(cgm_data_file, parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "\n",
    "    if \".xls\" in insulin_data_file:\n",
    "        insulin_df = pd.read_excel(insulin_data_file, parse_dates=[['Date', 'Time']])\n",
    "    elif \".csv\" in insulin_data_file:\n",
    "        insulin_df = pd.read_csv(insulin_data_file, parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "\n",
    "    cp_df = cgm_df[['Date_Time', 'Sensor Glucose (mg/dL)']]   # Filter only the needed fields to cp_df dataframe\n",
    "    cp_df = cp_df.set_index(['Date_Time'])\n",
    "    cp_df.sort_index(inplace=True)\n",
    "\n",
    "    cp_ins_df = insulin_df[['Date_Time', 'BWZ Carb Input (grams)']]   # Filter only the needed fields to cp_ins_df dataframe\n",
    "\n",
    "    # extract rows with Carb/meal intake values > 0\n",
    "    meal_intake_rows = cp_ins_df.loc[cp_ins_df['BWZ Carb Input (grams)'] > 0, ['Date_Time', 'BWZ Carb Input (grams)']]\n",
    "    meal_intake_rows.sort_values(['Date_Time'], inplace=True)\n",
    "    meal_intake_rows.reset_index(inplace=True)\n",
    "    meal_intake_rows.drop('index', inplace=True, axis=1)\n",
    "    # 'BWZ Carb Input (grams)'\n",
    "    valid_meal_data_times = meal_intake_rows\n",
    "    # print(valid_meal_data_times.shape)\n",
    "\n",
    "    rows_to_drop = []\n",
    "    last_date = valid_meal_data_times['Date_Time'][0]-timedelta(hours=10)\n",
    "\n",
    "    for ind, row in valid_meal_data_times.iterrows():\n",
    "        if row['Date_Time'] < (last_date+timedelta(hours=4)):\n",
    "            rows_to_drop.append(ind-1)\n",
    "        last_date = row['Date_Time']\n",
    "        \n",
    "    valid_meal_data_times.drop(rows_to_drop, inplace=True)\n",
    "    valid_meal_data_times.reset_index(inplace=True)\n",
    "    # print(valid_meal_data_times.shape)\n",
    "\n",
    "    ####Extract Meal and No_meal window data\n",
    "    meal_data = pd.DataFrame()\n",
    "    no_meal_data = pd.DataFrame()\n",
    "\n",
    "    for ind, row in valid_meal_data_times.iterrows():\n",
    "        # meal_time window data\n",
    "        m_data = cp_df[row['Date_Time']-timedelta(minutes=30):row['Date_Time']+timedelta(hours=2)]\n",
    "        # no_meal_time window data\n",
    "        n_m_data = cp_df[row['Date_Time']+timedelta(hours=2):row['Date_Time']+timedelta(hours=4)]\n",
    "\n",
    "        m_data.reset_index(inplace=True)\n",
    "        n_m_data.reset_index(inplace=True)\n",
    "\n",
    "        # m_data.fillna(0)\n",
    "        # n_m_data.fillna(0)\n",
    "\n",
    "        # Avoid meal and no_meal data instances with less than 30 and 24 observations respectively on a particular time window\n",
    "        # Avoid instances with more than 5 NaN values\n",
    "        # if (len(m_data) >= 30) and (m_data['Sensor Glucose (mg/dL)'][:30].isna().sum() <= 5):\n",
    "        if (len(m_data) >= 30):\n",
    "            meal_data = pd.concat([meal_data, m_data['Sensor Glucose (mg/dL)'][:30]], ignore_index=True, axis=1)\n",
    "        \n",
    "        # if (len(n_m_data) >= 24) and (n_m_data['Sensor Glucose (mg/dL)'][:24].isna().sum() <= 5):\n",
    "        if (len(n_m_data) >= 24):\n",
    "            no_meal_data = pd.concat([no_meal_data, n_m_data['Sensor Glucose (mg/dL)'][:24]], ignore_index=True, axis=1)\n",
    "        \n",
    "    # print(meal_data.shape)\n",
    "    # print(no_meal_data.shape)\n",
    "    meal_data = meal_data.transpose()\n",
    "    # meal_data.to_csv(\"meal.csv\")\n",
    "    no_meal_data = no_meal_data.transpose()\n",
    "\n",
    "    # print(f\"Meal Data shape: {meal_data.shape}\")\n",
    "    # print(f\"NoMeal Data Shape: {no_meal_data.shape}\")\n",
    "\n",
    "    return [meal_data, no_meal_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION - MEAL & NO MEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSlope(series):\n",
    "    res = np.polyfit(range(len(series)), series, 1)\n",
    "    # print(f\"All Results: {res}\")\n",
    "    return res[0]\n",
    "\n",
    "def extract_features(data_matrix, features):\n",
    "\n",
    "    feature_matrix = pd.DataFrame(columns=features)\n",
    "    # no_meal_feature = pd.DataFrame(columns=features)\n",
    "\n",
    "    slope_sampling_size = 2\n",
    "\n",
    "    # for di, dat in enumerate([meal_data, no_meal_data]):\n",
    "    #     f_idx = 0\n",
    "    for ind, data in data_matrix.iterrows():\n",
    "        # Max-Min Distance Feature\n",
    "        f1_diff = data.max() - data.min()\n",
    "\n",
    "        # Slope feature\n",
    "        slope_res = data.rolling(slope_sampling_size).apply(calcSlope)\n",
    "        zero_crossings = np.where(np.diff(np.sign(slope_res)))[0]   # Zero crossing indexes of slope\n",
    "        zero_crossings = np.hstack([zero_crossings, np.array(len(slope_res)-1)])\n",
    "        zero_cross_dist_df = pd.DataFrame(columns=['cross_index', 'distance'])\n",
    "        zc_idx = 0\n",
    "        for idx, slope_idx in enumerate(zero_crossings):\n",
    "            if (idx < 2) or (idx == (len(zero_crossings)-1)):\n",
    "                pass\n",
    "            else:\n",
    "                # Calculate the dist between Max and Min slopes on either sides of a zero crossing\n",
    "                # Max and Min sides depends on the sign of slope at zero crossing (if '-', the curve is increasing (Max->right, Min->left) and vice versa)\n",
    "                if slope_res[slope_idx] < 0:\n",
    "                    dist = max(slope_res[slope_idx:zero_crossings[idx+1]+1]) - min(slope_res[zero_crossings[idx-1]:slope_idx+1])\n",
    "                else:\n",
    "                    dist = max(slope_res[zero_crossings[idx-1]:slope_idx+1]) - min(slope_res[slope_idx:zero_crossings[idx+1]+1])\n",
    "                zero_cross_dist_df.loc[zc_idx] = [slope_idx, dist]\n",
    "                zc_idx += 1\n",
    "\n",
    "        zero_cross_dist_df.sort_values(['distance'], inplace=True, ascending=False)\n",
    "        zero_cross_dist_df.reset_index(inplace=True)\n",
    "\n",
    "        f2_slope_zero_cross_ordered_dist = []\n",
    "        len_zero_cross_dist_df = len(zero_cross_dist_df)\n",
    "\n",
    "        if len_zero_cross_dist_df < 3:\n",
    "            # Skip data that has less than three slope zero crossings\n",
    "            continue\n",
    "\n",
    "        for i in range(3):\n",
    "            f2_slope_zero_cross_ordered_dist.append(zero_cross_dist_df.loc[i, ['distance']].values[0])\n",
    "            f2_slope_zero_cross_ordered_dist.append(zero_cross_dist_df.loc[i, ['cross_index']].values[0])\n",
    "\n",
    "        # for i in range(3):\n",
    "        #     if i >= len_zero_cross_dist_df:\n",
    "        #         f2_slope_zero_cross_ordered_dist.append(0)\n",
    "        #         f2_slope_zero_cross_ordered_dist.append(0)\n",
    "        #     else:\n",
    "        #         f2_slope_zero_cross_ordered_dist.append(zero_cross_dist_df.loc[i, ['distance']].values[0])\n",
    "        #         f2_slope_zero_cross_ordered_dist.append(zero_cross_dist_df.loc[i, ['cross_index']].values[0])\n",
    "            \n",
    "        # Max-Min Value Index range/distance Feature\n",
    "        f3_slot_diff = abs(data.idxmax() - data.idxmin())\n",
    "\n",
    "        # Frequency Domain Feature\n",
    "        ## Normalize data\n",
    "        norm_data = data - data.mean()\n",
    "\n",
    "        yf = rfft(norm_data.values)\n",
    "        xf = rfftfreq(len(norm_data))\n",
    "        yf = np.abs(yf)\n",
    "        \n",
    "        ## Extract peaks\n",
    "        peak_idxs, _ = find_peaks(yf)\n",
    "        peaks = yf[peak_idxs]\n",
    "        peaks.sort()\n",
    "        peaks = peaks[::-1]\n",
    "        f4_freq_domain = list(peaks[:3])\n",
    "        if len(f4_freq_domain) < 3:\n",
    "            # Skip data that has less than 3 frequency peaks after FFT\n",
    "            continue\n",
    "        #     for i in range(3-len(f4_freq_domain)):\n",
    "        #         f4_freq_domain.append(0)\n",
    "\n",
    "        feature_matrix.loc[ind] = [f1_diff] + f2_slope_zero_cross_ordered_dist + [f3_slot_diff] + f4_freq_domain\n",
    "            \n",
    "    # print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    return feature_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(dataframe, get_attributes=False, mean_data=None, max_min_diff=None):\n",
    "    if get_attributes:\n",
    "        mean_data = dataframe.mean(axis=0)\n",
    "        # print(\"MEAN DATA:\")\n",
    "        # print(mean_data)\n",
    "        max_data = dataframe.max(axis=0)\n",
    "        # print(\"MAX DATA:\")\n",
    "        # print(max_data)\n",
    "        min_data = dataframe.min(axis=0)\n",
    "        # print(\"MIN DATA:\")\n",
    "        # print(min_data)\n",
    "        max_min_diff = max_data-min_data\n",
    "        # print(\"MAX-MIN DATA:\")\n",
    "        # print(max_min_diff)\n",
    "        dataframe = (dataframe - mean_data)/(max_min_diff - (2 * mean_data))\n",
    "        # normRawData = (rY - numpy.mean(rY))/(numpy.max(rY-numpy.mean(rY))-numpy.min(rY-numpy.mean(rY)))\n",
    "        # print(dataframe)\n",
    "        return (dataframe, mean_data, max_min_diff)\n",
    "    else:\n",
    "        dataframe = (dataframe - mean_data)/(max_min_diff - (2 * mean_data))\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cgm_df = pd.read_csv(\"data/CGMData.csv\", parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "# insulin_df = pd.read_csv(\"data/InsulinData.csv\", parse_dates=[['Date', 'Time']], low_memory=False)\n",
    "\n",
    "\n",
    "# Patient 2\n",
    "# cgm_df = pd.read_excel(\"data/CGMData670GPatient3.xlsx\", parse_dates=[['Date', 'Time']])\n",
    "# insulin_df = pd.read_excel(\"data/InsulinAndMealIntake670GPatient3.xlsx\", parse_dates=[['Date', 'Time']])\n",
    "\n",
    "def split_train_test_tests(meal_feature, no_meal_feature, features):\n",
    "    ## TRAIN and TEST Splits\n",
    "\n",
    "    # meal train_test_split() - Train: 80%, Test: 20%\n",
    "    meal_x_train, meal_x_test, meal_y_train, meal_y_test = train_test_split(meal_feature.loc[:, features], meal_feature.loc[:, [target]], train_size=0.9)\n",
    "\n",
    "    # no_meal train_test_split() - Train: 80%, Test: 20%\n",
    "    no_meal_x_train, no_meal_x_test, no_meal_y_train, no_meal_y_test = train_test_split(no_meal_feature.loc[:, features], no_meal_feature.loc[:, [target]], train_size=0.9)\n",
    "\n",
    "    # Combined Train Set\n",
    "    meal_train = np.hstack([meal_x_train, meal_y_train])\n",
    "    no_meal_train = np.hstack([no_meal_x_train, no_meal_y_train])\n",
    "\n",
    "    all_feature_train_matrix = np.vstack([meal_train, no_meal_train])\n",
    "    all_feature_train_matrix = pd.DataFrame(all_feature_train_matrix, columns=features+[target])\n",
    "\n",
    "    # Combined Test Set\n",
    "    meal_test = np.hstack([meal_x_test, meal_y_test])\n",
    "    no_meal_test = np.hstack([no_meal_x_test, no_meal_y_test])\n",
    "\n",
    "    all_feature_test_matrix = np.vstack([meal_test, no_meal_test])\n",
    "    all_feature_test_matrix = pd.DataFrame(all_feature_test_matrix, columns=features+[target])\n",
    "    # print(all_feature_train_matrix.shape)\n",
    "\n",
    "\n",
    "    x_train = all_feature_train_matrix.loc[:, features].values\n",
    "    y_train = all_feature_train_matrix.loc[:, [target]].values\n",
    "\n",
    "    x_test = all_feature_test_matrix.loc[:, features].values\n",
    "    y_test = all_feature_test_matrix.loc[:, [target]].values\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, classifier, pca):\n",
    "\n",
    "    # # Standardization\n",
    "\n",
    "    # x_train, feature_mean, feature_max_min_diff = standardize(x_train, get_attributes = True)\n",
    "\n",
    "    # # PCA\n",
    "\n",
    "    # pca.fit(x_train)\n",
    "\n",
    "    # To be used for model train & test\n",
    "    train_pca = pca.transform(x_train)\n",
    "\n",
    "    classifier.fit(train_pca, y_train.ravel())\n",
    "\n",
    "    # return (feature_mean, feature_max_min_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(x_test, y_test, feature_mean, feature_max_min_diff, classifier, pca):\n",
    "    # Standardize\n",
    "    x_test = standardize(x_test, False, feature_mean, feature_max_min_diff)\n",
    "\n",
    "    # PCA\n",
    "    test_pca = pca.transform(x_test)\n",
    "\n",
    "    y_pred = classifier.predict(test_pca)\n",
    "\n",
    "    # print(y_pred)\n",
    "\n",
    "    print(\"Accuracy : \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_label_sets(data_matrix_1, data_matrix_2, label, target):\n",
    "\n",
    "    train_data_1 = data_matrix_1.loc[:, features].to_numpy()\n",
    "    train_data_2 = data_matrix_2.loc[:, features].to_numpy()\n",
    "\n",
    "    train_label_1 = data_matrix_1.loc[:, [target]].to_numpy()\n",
    "    train_label_2 = data_matrix_2.loc[:, [target]].to_numpy()\n",
    "\n",
    "    x_train = np.vstack([train_data_1, train_data_2])\n",
    "    y_train = np.vstack([train_label_1, train_label_2])\n",
    "\n",
    "    return [x_train, y_train]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROMAT: [cgm_data, insulin_data]\n",
    "training_input_files = [[\"data/CGMData.csv\", \"data/InsulinData.csv\"], [\"data/CGMData670GPatient3.xlsx\", \"data/InsulinAndMealIntake670GPatient3.xlsx\"]]\n",
    "\n",
    "meal_train_data_matrix_1, no_meal_train_data_matrix_1 = extract_meal_and_no_meal_instances(training_input_files[0][0], training_input_files[0][1])\n",
    "meal_train_data_matrix_2, no_meal_train_data_matrix_2 = extract_meal_and_no_meal_instances(training_input_files[1][0], training_input_files[1][1])\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "pca = PCA()\n",
    "\n",
    "features = ['f1_diff', 'f2_slope_cross_dist_1', 'f2_slope_cross_slot_1', 'f2_slope_cross_dist_2', 'f2_slope_cross_slot_2', 'f2_slope_cross_dist_3', 'f2_slope_cross_slot_3', 'f3_slot_diff', 'f4_dom_freq_1', 'f4_dom_freq_2', 'f4_dom_freq_3']\n",
    "\n",
    "# features = ['f1_diff', 'f2_slope_cross_dist_1', 'f2_slope_cross_slot_1', 'f3_slot_diff', 'f4_dom_freq_1']\n",
    "\n",
    "# # Use two data to train and test\n",
    "# meal_feature_1 = extract_features(meal_train_data_matrix_1, features)\n",
    "# no_meal_feature_1 = extract_features(no_meal_train_data_matrix_1, features)\n",
    "\n",
    "# target = 'is_meal'\n",
    "# meal_feature_1[target] = 1\n",
    "# no_meal_feature_1[target] = 0\n",
    "\n",
    "# meal_feature_2 = extract_features(meal_train_data_matrix_2, features)\n",
    "# no_meal_feature_2 = extract_features(no_meal_train_data_matrix_2, features)\n",
    "\n",
    "# target = 'is_meal'\n",
    "# meal_feature_2[target] = 1\n",
    "# no_meal_feature_2[target] = 0\n",
    "\n",
    "# x_train_1, y_train_1, x_test_1, y_test_1 = split_train_test_tests(meal_feature_1, no_meal_feature_1, features)\n",
    "# x_train_2, y_train_2, x_test_2, y_test_2 = split_train_test_tests(meal_feature_2, no_meal_feature_2, features)\n",
    "\n",
    "# # Standardization\n",
    "\n",
    "# x_train_1, feature_mean_1, feature_max_min_diff_1 = standardize(x_train_1, get_attributes = True)\n",
    "\n",
    "# # PCA\n",
    "\n",
    "# pca.fit(x_train_1)\n",
    "\n",
    "# # Standardization\n",
    "\n",
    "# x_train_2, feature_mean_2, feature_max_min_diff_2 = standardize(x_train_2, get_attributes = True)\n",
    "\n",
    "# # PCA\n",
    "\n",
    "# pca.fit(x_train_2)\n",
    "\n",
    "# # feature_mean_1, feature_max_min_diff_1 = train_model(x_train_1, y_train_1, classifier, pca)\n",
    "# # feature_mean_2, feature_max_min_diff_2 = train_model(x_train_2, y_train_2, classifier, pca)\n",
    "\n",
    "# train_model(x_train_1, y_train_1, classifier, pca)\n",
    "# train_model(x_train_2, y_train_2, classifier, pca)\n",
    "\n",
    "# feature_mean = (feature_mean_1 + feature_mean_2)/2\n",
    "# feature_max_min_diff = (feature_max_min_diff_1 + feature_max_min_diff_2)/2\n",
    "\n",
    "# test_model(x_test_1, y_test_1, feature_mean, feature_max_min_diff, classifier, pca)\n",
    "# test_model(x_test_2, y_test_2, feature_mean, feature_max_min_diff, classifier, pca)\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Combine 2 patient data for training\n",
    "meal_feature_1 = extract_features(meal_train_data_matrix_1, features)\n",
    "no_meal_feature_1 = extract_features(no_meal_train_data_matrix_1, features)\n",
    "\n",
    "meal_feature_2 = extract_features(meal_train_data_matrix_2, features)\n",
    "no_meal_feature_2 = extract_features(no_meal_train_data_matrix_2, features)\n",
    "\n",
    "meal_feature_1 = pd.concat([meal_feature_1, meal_feature_2])\n",
    "no_meal_feature_1 = pd.concat([no_meal_feature_1, no_meal_feature_2])\n",
    "\n",
    "# print(meal_feature_1.shape)\n",
    "# print(no_meal_feature_1.shape)\n",
    "\n",
    "target = 'is_meal'\n",
    "meal_feature_1[target] = 1\n",
    "no_meal_feature_1[target] = 0\n",
    "\n",
    "\n",
    "# x_train_1, y_train_1, x_test_1, y_test_1 = split_train_test_tests(meal_feature_1, no_meal_feature_1, features)\n",
    "\n",
    "x_train_1, y_train_1 = split_data_label_sets(meal_feature_1, no_meal_feature_1, features, target)\n",
    "\n",
    "# print(x_train_1)\n",
    "\n",
    "# Standardization\n",
    "\n",
    "x_train_1, feature_mean_1, feature_max_min_diff_1 = standardize(x_train_1, get_attributes = True)\n",
    "\n",
    "# print(feature_mean_1)\n",
    "# print(feature_max_min_diff_1)\n",
    "\n",
    "# PCA\n",
    "# print(x_train_1)\n",
    "pca.fit(x_train_1)\n",
    "\n",
    "\n",
    "# feature_mean_1, feature_max_min_diff_1 = train_model(x_train_1, y_train_1, classifier, pca)\n",
    "train_model(x_train_1, y_train_1, classifier, pca)\n",
    "\n",
    "# test_model(x_test_1, y_test_1, feature_mean_1, feature_max_min_diff_1, classifier, pca)\n",
    "\n",
    "# Pickle all needed objects\n",
    "\n",
    "obj_dict = {'classifier': classifier, 'pca': pca, 'feature_mean': feature_mean_1, 'feature_max_min_diff': feature_max_min_diff_1}\n",
    "\n",
    "filename = 'model.pkl'\n",
    "outfile = open(filename, 'wb')\n",
    "\n",
    "pickle.dump(obj_dict, outfile)\n",
    "outfile.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dff2a0c761797010374bfd188fea3b2bf81132b9f6e1a233ed5ea24c4498355"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
